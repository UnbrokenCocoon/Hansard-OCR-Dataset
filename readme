# ğŸ“œ Hansard OCR Dataset Builder

This project provides a full pipeline for downloading and preparing historical UK parliamentary debates (Hansard) for use in OCR training â€” particularly for applications in **digital humanities**, **historical NLP**, and **document layout modelling**.

---

## ğŸ“‚ What This Project Does

* âœ… Downloads the complete digitised Hansard archive from 1800â€“2004 using a precompiled list of ZIP archive URLs
* âœ… Extracts and converts debate text into **synthetic, book-style page images** paired with ground truth text
* âœ… Supports augmentation and "denigration" (noise injection) to simulate historical document imperfections
* âœ… Designed for OCR model fine-tuning, including projects using TrOCR or LayoutLM
* âš ï¸ **Large-scale**: Over 2,000 ZIPs, potentially hundreds of thousands of pages

---

## ğŸ“Ÿ Dataset Source

We use the ZIP archive list compiled by [econandrew/uk-hansard-archive-urls](https://github.com/econandrew/uk-hansard-archive-urls). We thank the author for this essential index of the digitised Hansard archive.

The raw ZIP archives contain HTML-structured parliamentary debates across both houses of Parliament. This data is in the public domain.

---

## ğŸ“… Scripts Overview

### `hansard_fetcher.py`

* Downloads all ZIP archives into `hansard_zips/`
* Skips existing files (safe to resume)
* Fetches from the `urls.txt` file in the referenced GitHub repo

### `dataset_creator.py`

* Extracts HTML from ZIPs
* Parses into column-paragraph structure
* Chunks into \~300-token blocks
* Renders text into synthetic A5-style page images using historical-style fonts
* Applies light noise, skew, and blur to simulate OCR input quality degradation
* Saves `.png + .txt` pairs in an output folder

### `train_trocr.py`

* Fine-tunes a TrOCR model on the synthetic image-text pairs
* Supports the HuggingFace `Seq2SeqTrainer` setup
* Includes options for mixed precision training, logging, evaluation, and model saving
* Generates a fine-tuned OCR model ready for inference on historical scans

---

## ğŸ§  Intended Use

This dataset is ideal for:

* ğŸ•µï¸â€â™‚ï¸ OCR model training (e.g. TrOCR, Tesseract, LayoutLM)
* ğŸ“š Digital humanities and historical research
* ğŸ“œ Document layout and semantic structure modelling
* ğŸ’¡ Synthetic dataset generation at scale for benchmark creation

---

## ğŸ“Š Benefits

* Public domain: fully open for research and commercial use
* Historically rich: authentic political, rhetorical, and linguistic diversity
* Reproducible: built with plain Python, no dependency on proprietary formats
* Scalable: millions of lines of ground-truth-aligned content

---

## ğŸ”§ Next Steps

1. Run `hansard_fetcher.py` to download all source data
2. Use `dataset_creator.py` to:

   * Extract and clean content
   * Generate synthetic A5-style page images
   * Apply stochastic degradation (augmentation)
3. Use `train_trocr.py` to fine-tune a TrOCR model on your dataset
4. Apply the model to real-world historical scans for OCR prediction

---

## ğŸ“… License

The Hansard data is public domain (UK Crown Copyright).
Code in this repository is released under the MIT License.

---

## ğŸ¤ Acknowledgements

* Archive URLs: [https://github.com/econandrew/uk-hansard-archive-urls](https://github.com/econandrew/uk-hansard-archive-urls)
* Hansard archives provided by: UK Parliament Digital Archive
