"""
ğŸ§¾ Script: create_ocr_dataset.py
ğŸ“¦ Purpose: Build and save a Hugging Face-compatible OCR dataset from imageâ€“text pairs
"""

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
import os, glob
from PIL import Image

from datasets import (
    Dataset,
    DatasetDict,
    Features,
    Value,
)
from datasets.features import Image as HFImage

from sklearn.model_selection import train_test_split

from transformers import (
    TrOCRProcessor,
    VisionEncoderDecoderModel,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    default_data_collator,
)
from torch.utils.data import DataLoader

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. USER PATHS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
root_dir  = r"path/to/your/dir"              # â† folder holding the .jpg/.txt pairs
save_path = r"path/to/your/dir"  # â† where the processed dataset will live

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. PROCESSOR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
processor = TrOCRProcessor.from_pretrained(
    "microsoft/trocr-base-stage1",
    use_fast=True                      # kills the â€œslow processorâ€ warning
)

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. COLLECT PAIRS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
def collect_examples(folder):
    examples = []
    for dirpath, _, _ in os.walk(folder):
        for jpg_path in glob.glob(os.path.join(dirpath, "*.jpg")):
            txt_path = jpg_path[:-4] + ".txt"
            if os.path.exists(txt_path):
                with open(txt_path, encoding="utf-8") as f:
                    text = f.read().strip()
                if text:
                    examples.append({"image": jpg_path, "text": text})
    return examples

data = collect_examples(root_dir)
print(f"âœ… collected {len(data)} imageâ€“text pairs")

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. BUILD DATASET WITH IMAGE TYPE â”€â•®
features = Features({"image": HFImage(), "text": Value("string")})
raw_dataset = Dataset.from_list(data, features=features)

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. TRAIN/TEST SPLIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
train_ds, test_ds = train_test_split(raw_dataset, test_size=0.1, random_state=42)
dataset = DatasetDict(train=train_ds, test=test_ds)
print(dataset)

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. PREPROCESS (lazy â†’ tensors) â”€â”€â”€â•®
def preprocess(batch):
    imgs   = [img.convert("RGB") for img in batch["image"]]  # HF passes PILs
    texts  = batch["text"]
    enc    = processor(images=imgs, text=texts,
                       truncation=True, padding="max_length",
                       max_length=128, return_tensors="pt")
    enc["labels"][enc["labels"] == processor.tokenizer.pad_token_id] = -100
    return {"pixel_values": enc["pixel_values"], "labels": enc["labels"]}

dataset = dataset.map(preprocess, batched=True, remove_columns=["image", "text"])
dataset.save_to_disk(save_path)
print("ğŸ’¾ saved cleaned dataset to", save_path)

train_dataset = dataset["train"]
eval_dataset  = dataset["test"]

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. DATALOADERS (optional) â”€â”€â”€â”€â”€â”€â”€â”€â•®
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True,
                              collate_fn=default_data_collator)
eval_dataloader  = DataLoader(eval_dataset , batch_size=8,
                              collate_fn=default_data_collator)

# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. MODEL & TRAINING SETUP â”€â”€â”€â”€â”€â”€â”€â”€â•®
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-stage1")
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id           = processor.tokenizer.pad_token_id
model.config.vocab_size             = model.decoder.config.vocab_size

training_args = Seq2SeqTrainingArguments(
    output_dir          = "./trocr-ocr-model",
    eval_strategy = "epoch",
    save_strategy       = "epoch",
    per_device_train_batch_size = 8,
    per_device_eval_batch_size  = 8,
    num_train_epochs    = 5,
    fp16                = False,   # set True if your GPU supports it
    logging_dir         = "./logs",
    logging_steps       = 10,
    save_total_limit    = 2,
    predict_with_generate = True,
    remove_unused_columns = False, # keep pixel_values/labels
)

trainer = Seq2SeqTrainer(
    model         = model,
    args          = training_args,
    train_dataset = train_dataset,
    eval_dataset  = eval_dataset,
    processor     = processor,           # new-style arg
    data_collator = default_data_collator,
)

print("ğŸš€ ready to run trainer.train()")

#trainer.train()
